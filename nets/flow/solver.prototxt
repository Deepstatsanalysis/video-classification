net: "net.prototxt"
# Do not test before training
test_initialization: true 
# Test every nth iteration
test_interval: 2000
# How many iterations per test
test_iter: 200

# Base learning rate
base_lr: 0.001
# Policy for changing the learning rate - multiply by gamma every stepsize iterations
lr_policy: "step"
gamma: 0.1
stepsize: 28000
momentum: 0.9
# Regularization parameter for the weights
weight_decay: 0.00001

# Display training loss every nth iteration
display: 200
# After how many iterations to stop
max_iter: 1000000

# Snapshot every nth iteration in the specified directory
snapshot: 10000
snapshot_prefix: "snapshots/"

random_seed: 1701
# Display the loss averaged over the last average_loss iterations - this does not work for accuracy
average_loss: 100
#clip_gradients: 10

# GPU for the win!
solver_mode: GPU
