net: "net.prototxt"
# Test every nth iteration
test_interval: 8000
# How many iterations per test
test_iter: 800
# Base learning rate
base_lr: 0.001

# Policy for changing the learning rate - multiply by gamma every stepsize iterations
lr_policy: "step"
gamma: 0.5
stepsize: 50000
momentum: 0.9
# Regularization parameter for the weights
weight_decay: 0.0005

# Display training loss every nth iteration
display: 100
# After how many iterations to stop
max_iter: 1000000

# Snapshot every nth iteration in the specified directory
snapshot: 5000
snapshot_prefix: "./snapshots/"

random_seed: 1701
# Display the loss averaged over the last average_loss iterations - this does not work for accuracy
average_loss: 100
#clip_gradients: 10

# GPU for the win!
solver_mode: GPU
