%!TEX root = ../paper.tex
\section{Video Classification in Caffe}
\label{sec:classification}

For building our artificial neural network we used the framework Caffe\footnote{\url{http://caffe.berkeleyvision.org/}}.
Caffe is a deep learning framework offering several implementations of layers.
Besides, Caffe is GitHub project and can be adapted and extended for special use cases.
We used with the Caffe branch of Jeff Donahue\footnote{\url{https://github.com/BVLC/caffe/pull/2033}}, which offers an implementation for long-short-term memory (LSTM).

The architecture of our artificial neural network was build on the work of TODO.
This architecture consists of three parts and can be found in Figure~\ref{fig:architecture}.
The first part is processing the single frames of a video, further called \emph{spatial}.
It is responsible for recognizing objects and structures in frames.
The second part memorizes motion of actions.
It handles flow images and is therefore called \emph{flow}.
Both parts consists of a convolutional neural network (CNN) followed by a long-short-term memory (LSTM).
To merge the predictions of both parts, \emph{spatial} and \emph{flow}, a third part is introduced.
This part, called \emph{fusion}, takes the output of both CNNs, \emph{spatial} and \emph{flow}, and merge the predictions.
To get a final overall prediction the three results are combined in the end.

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=.7]{images/architecture.eps}
	\caption{General Architecture TODO}
	\label{fig:architecture}
\end{figure}

The individual neural networks we used in this architecture differ from the work of TODO.
Also we adapt the architecture by removing the LSTMs, only having a CNN for the \emph{spatial} and \emph{flow} part.
This is because our results using LSTMs were no that good as using only CNNs.\todo{Correct?}
During our research we tried out different neural networks, which will be presented in this section.
Also the parameters, which were used for the training, and the results will be shown.

\subsection{Changes to Caffe}

Working with a neural network consisting of LSTMs requires to use two different inputs:
On the one hand the raw pixel values and on the other hand a tagging sequence.
The tagging sequence tells the LSTM, where a new training example starts, as we process more than one training sequence per batch.
A tagging sequence consist of 0's and 1's.
A 1 indicates the beginning of a new sequence.
For generating this tagging sequence we wrote a new layer of type \emph{DummyData}.
An example can be found in Listing~\ref{lst:seq-layer}.
The layer has the two parameters:
The first parameter is the shape.
The first dimension of the shape is the length of a sequence, the second dimension is the number of sequences fitting into one batch.
The second parameter ...\todo{???}

\begin{lstlisting}[language=sh, caption=Sequence Layer, label=lst:seq-layer]
layer {
	name: "sequence"
	type: "DummyData"
	top: "sequence"
	dummy_data_param {
		shape {
			dim: 16
			dim: 4
		}
		data_filler {
			type: "sequence"
			value: 16
		}
	}
}
\end{lstlisting}

A second adaption we made to Caffe was the implementation of the script \emph{convert\_imageset\_multi}, which was already introduced in Section~\ref{sec:data}.

% \begin{itemize}
% 	\item New sequence data layer
% 	\item Multi-layer script
% 	\item More?
% \end{itemize}

\subsection{Spatial}
\label{subsec:spatial}

\begin{itemize}
	\item
		Different nets:
		\begin{itemize}
			\item Caffenet/CNN\_M (also tried, VGG 19, but too big)
			\item With weights/without weights
			\item Compare the nets with respect to memory, number of parameters, training time, performance
		\end{itemize}
	\item
		Experiments:
		\begin{itemize}
			\item Different dropouts
			\item Train from scratch vs train from weights
			\item On different splits?
			\item Fc6, Fc7, fc8
			\item Different base data sets (only 16, all data)
			\item Different flows?
			\item Occlusion tests
		\end{itemize}
	\item
		LSTM did not work out
\end{itemize}


\subsection{Flow}
\label{subsec:flow}



\subsection{Fusion}
\label{subsec:fusion}
As first approach we rebuild the fusion architecture presented by TODO.

We took the CNN nets for spatial and flow presented above and build a fusion architecture on top of them.
Both CNNs were cut off after the \textit{fc6}-layer having an output of 16 x 4096.\todo{Der eigentlich output von fc6 ist ja erstmal 1 x 4096}
The input to the CNNs were 16 frames per video.
So the output of the \textit{fc6}-layer of the spatial and flow net correspond to a prediction for each of those frames.
To fuse those predictions, the first step is to merge the 16 prediction of one video into one prediction for the whole video.
Therefore, we take the average prediction for both spatial and flow.
A fully-connected layer is then trained with those predictions before we concatenate the predictions of spatial and flow.
In the end two fully-connected layer are trained on the merged predictions.
The output is defined by an accuracy layer.
The whole architecture is shown in Figure~\ref{fig:fusion_architecture}\todo{improve figure}.
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=.7]{images/fusion_architecture.eps}
	\caption{Architecture of the fusion net: The predictions per frame of one video from the spatial and flow net are merged into one prediction per video each. Those predictions are then merged and trained via two fully connected layers.}
	\label{fig:fusion_architecture}
\end{figure}






