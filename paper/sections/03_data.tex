\section{Data Preparation}
\label{sec:data}

As mentioned in Section~\ref{sec:related}, there are many parameters in the data generation and preparation process.
In general we first extract the frame images from a video, and from these the optical flow images, which show movement in the video.
After the data generation the preprocessed data has to be converted into a suitable data file for the Caffe framework.
The best results were achieved using the following setup:
\begin{itemize}
	\item Frame extraction with 10 frames per second (\todo[inline]{is this correct?} copy into subsection below)
	\item Do not crop videos ourselves, rather use Caffe's random cropping
	\item Stack 20 frames. For each frame, take the next 10 optical flow frames
	\item Optical flow was calculated with OpenCV's broxoptflow~\cite{brox2004high}
\end{itemize}
In the following section we will discuss the different ways and data formats available for each step in detail.

\subsection{Dataset}
For our research purposes we relied on UFC101~\cite{soomro2012ucf101} dataset, a popular choice in the computer vision community.
UCF101 is an action recognition data set of realistic action videos, collected from YouTube, having 101 action categories.
The videos in 101 action categories are grouped into 25 groups, where each group can consist of 4-7 videos of an action.
The videos from the same group may share some common features, such as similar background, similar viewpoint, etc.
The action categories can be divided into five types:
\begin{enumerate}
\item Human-Object Interaction
\item Body-Motion Only
\item Human-Human Interaction
\item Playing Musical Instruments
\item Sports
\end{enumerate}
The dataset contains 13320 clips with a fixed frame rate and resolution of 25 FPS and \texttt{320 x 240} respectively.
There are also a few videos in the \emph{Pommelhorse} category, which seem to have a different resolution of \texttt{400 x }\todo{??}.
We dealt with these by rescaling them to \texttt{320 x 240}.\todo{We did?}
On average the clips have a length of 7 seconds.

For comparable results the dataset's authors published three fixed train/test splits.
We worked on the first split, i.e. the first thirds were used for training, and the last one was used for testing.
When working with UC101 it is important to follow these splits, because the training instances are often sampled and cut from a longer video sequence.
Concretely, this means that when generating the dataset, there usually was a long sequence of one activity.
This long sequence was then cut, to create more than one training sample from this sequence.
It is important to keep the samples from one original sequence in the same dataset.
Otherwise, the test performance is estimated to high.

%UCF101 is composed of unconstrained videos downloaded from YouTube which feature challenges such as poor lighting, cluttered background and severe camera motion.

\subsection{Frame extraction}
We extracted the frame data from the videos with the FFmpeg\footnote{\url{http://www.ffmpeg.org/}} tool into JPEG files.
A challenge was finding the correct frame rate for the frame extraction.
High frame rates, such as 30 frames per second, often extracted two frame images from the same unchanged video frame.
Lower frame rates, for example 5 frames per second, do not have these identical frame images.
However they create less overall training data and less details, especially for optical flow extraction.
A variable frame rate which is different for each video means there would likely be different results of analysis, depending on the frame rate a video was recorded with.
This is why we decided to use a fixed frame rate of X (TODO) frames per second.
This minimized the occurrence of two identical frames after each other while still giving a sufficient amount of detail.

\subsection{Optical flow extraction}
\todo{Figure}
Using the motions to deduce meaning out of a video with a convolutional neural network can be done with optical flow images.
They are calculated between each pair of consecutive frame images.

There are multiple ways to extract the optical flow but previous researchers \todo{reference} have often decided for the algorithm presented by Brox et al
\cite{brox2004high}.



\subsection{Frame stacking}
\todo[inline]{20, 10, optical flow forward/backward, which ones did we try?}


\subsection{Data format for caffe}
% Experiences with LMDB, LevelDB, HDF5
Caffe requires the input data to be in either a LMDB (TODO), HDF5, or LevelDB.

In our initial tests, we used LMDB, as this is usually used in the Caffe examples, and seems to be the recommended option.

The problem with LMDB was that it created large databases, because no compression is used.
We then decided to use HDF5, which compresses the data with the TODO algorithm.
However, the library we used .. TODO .. cannot be recommended.

Finally we used LevelDB,

%Table:
%Creation Time
%Database Size


\todo[inline]{Add some numbers about file sizes and creation time.}

\subsection{convert\_imageset}
The script \texttt{convert\_imageset} is a script provided by Caffe.
It creates either LMDBs or LevelDBs.
This can be switched by changing the constant \texttt{backend} in the script file and recompiling.

\subsection{convert\_imageset\_multi}
The script \texttt{convert\_imageset\_multi} is a script written by us, based on \texttt{convert\_imageset}.
It takes an additional parameter \texttt{STACK\_SIZE} at the end, which tells how many images from the list file to stack into one image.
Internally, it uses the method \texttt{cv::merge} from OpenCV to stack the images before writing them to the database.
